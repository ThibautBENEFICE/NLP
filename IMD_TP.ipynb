{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DlzvXLQuoTY"
      },
      "source": [
        "# Sentiment classifier with LSTM\n",
        "\n",
        "In this notebook, we will implement a simple sentiment classifier using an LSTM. Follow the notebook, complete the missing part, answer the questions and apply the asked modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utTvPhcyTC7d"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_v62rH_W9J1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import re\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzAHrPfReDqu"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "Using the datasets library, we load the imdb dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBb40sTOXYz-"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDq8WhU3XlOR"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kr3Qby4KWTZ6"
      },
      "outputs": [],
      "source": [
        "# We do not need the \"unsupervised\" split.\n",
        "dataset.pop(\"unsupervised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Hy5byjWTZ8"
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXssc1MjX5v2"
      },
      "outputs": [],
      "source": [
        "dataset[\"train\"][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UoMzhBOeGxe"
      },
      "source": [
        "## Pretreatment **(1 point)**\n",
        "\n",
        "Code the `pretreatment` function which clean the input text. Look at the dataset and deduce which treatment is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw3lht4AZHQB"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "\n",
        "def pretreatment(text: str) -> str:\n",
        "\t\"\"\"Clean IMDB text entries.\n",
        "    Args:\n",
        "        text: an input string.\n",
        "    Returns:\n",
        "        The cleaned text.\n",
        "    \"\"\"\n",
        "\t# Your code here\n",
        "\tlower = text.lower()\n",
        "\treturn lower.translate(str.maketrans('', '', punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnxsNRj1b4ef"
      },
      "outputs": [],
      "source": [
        "# This applies the pretreatment function to all\n",
        "clean_dataset = dataset.map(lambda x: {\"text\": pretreatment(x[\"text\"]), \"label\": x[\"label\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITYcafjvZ4V"
      },
      "source": [
        "Let's see what the text now look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ss9fNfoWUAz_"
      },
      "outputs": [],
      "source": [
        "clean_dataset[\"train\"][\"text\"][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sAscTs3U2Sn"
      },
      "source": [
        "Let's take a quick look at the labels. Notice that the labels are ordered in the training set starting by the negative reviews (0), followed by the positive ones (1). Training neural networks on this kind of configuration tends to considerably affect their performances. So the dataset will have to be shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZedAEQpdstA"
      },
      "outputs": [],
      "source": [
        "clean_dataset[\"train\"][\"label\"][12490:12510]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv3rt4ZieQbN"
      },
      "source": [
        "## Train/validation split **(1 point)**\n",
        "\n",
        "In our example, we consider the test split as production data. Which means, we need to treat it as if we never see it during the training process. To experiment on the model, we need to split the training set into a training and validation set. See [here](https://huggingface.co/course/chapter5/3?fw=pt#creating-a-validation-set) on how to do so with the `Datasets` library.\n",
        "\n",
        "Don't forget to **stratify** your split (we need to have the same proportion of class in both training and validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QIBkz-0YAO7"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "clean_dataset = DatasetDict({\n",
        "    'train': load_dataset('imdb', split='train[:40%]+train[50%:90%]'),\n",
        "    'validation': load_dataset('imdb', split='train[40%:50%]+train[90%:]'),\n",
        "    'test': clean_dataset['test']\n",
        "}).shuffle(seed=42)\n",
        "'''\n",
        "l = int(25000/2)\n",
        "perc = 0.80\n",
        "lperc= int(l*perc)\n",
        "\n",
        "train_text =clean_dataset['train']['text'][:lperc] +  clean_dataset['train']['text'][l:l+lperc]\n",
        "val_text = clean_dataset['train']['text'][lperc:l] +  clean_dataset['train']['text'][l+lperc:2*l]\n",
        "\n",
        "train_label=clean_dataset['train']['label'][:lperc] +  clean_dataset['train']['label'][l:l+lperc]\n",
        "val_label= clean_dataset['train']['label'][lperc:l] +  clean_dataset['train']['label'][l+lperc:2*l]\n",
        "\n",
        "train = {'text': train_text, 'label': train_label}\n",
        "val = {'text': val_text, 'label': val_label}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dataset['train']['label'][:10]"
      ],
      "metadata": {
        "id": "h3gqwi1hXhlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train,X_val,Y_train,Y_val = train_test_split(clean_dataset[\"train\"][\"text\"],clean_dataset[\"train\"][\"label\"],test_size=0.2,shuffle=True)"
      ],
      "metadata": {
        "id": "K5L5UYHlWTaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# sum(Y_train)"
      ],
      "metadata": {
        "id": "TJPgOxLiWTaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F58TBVUbVfWb"
      },
      "outputs": [],
      "source": [
        "clean_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w97hMnYEeexj"
      },
      "outputs": [],
      "source": [
        "# Check here that the dataset is stratified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t40J_xte1dz"
      },
      "source": [
        "## Categorical encoding of the vocabulary **(2 points)**\n",
        "\n",
        "We can't feed word to a neural network. A usual solution is to turn words into categorical data by using one-hot encoding. To avoid an explosion in vocabulary size, we will only keep words which appear more than a certain amount of time.\n",
        "\n",
        "The `Vocabulary` class below will do that for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2P9A4j8aarj"
      },
      "outputs": [],
      "source": [
        "UNK_TOKEN = \"<UNK>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "\t\"\"\"Vocabulary manager on a collection.\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self) -> None:\n",
        "\t\t\"\"\"No parameters to provide.\n",
        "        \"\"\"\n",
        "\t\t# Index to word mapping.\n",
        "\t\tself.index2word = [PAD_TOKEN, UNK_TOKEN]\n",
        "\t\t# Word to index mapping.\n",
        "\t\tself.word2index = {value: key for key, value in enumerate(self.index2word)}\n",
        "\t\t# Word counter.\n",
        "\t\tself.word2count = defaultdict(int)\n",
        "\n",
        "\tdef add_word(self, word: str) -> None:\n",
        "\t\t\"\"\"Increments the count of a word to the vocabulary.\n",
        "        Args:\n",
        "            word: the word.\n",
        "        \"\"\"\n",
        "\t\tself.word2count[word] += 1\n",
        "\t\tif not word in self.word2index:\n",
        "\t\t\tself.word2index[word] = len(self.index2word)\n",
        "\t\t\tself.index2word.append(word)\n",
        "\n",
        "\tdef add_text(self, text: str, separator: str = \" \") -> None:\n",
        "\t\t\"\"\"Add the words given in a text to our vocabulary.\n",
        "        Args:\n",
        "            text: a sequence of words separated by a given separator.\n",
        "            separator: the separator used to split our text (default is \" \").\n",
        "        \"\"\"\n",
        "\t\tfor word in text.split(separator):\n",
        "\t\t\tself.add_word(word)\n",
        "\n",
        "\tdef get_index(self, word: str) -> int:\n",
        "\t\t\"\"\"Returns the index of a given word in our vocabulary.\n",
        "        If the word is not in the vocabulary, returns the index for UNK_TOKEN.\n",
        "        Args:\n",
        "            word: a string.\n",
        "        Returns:\n",
        "            The corresponding index or the index for UNK_TOKEN.\n",
        "        \"\"\"\n",
        "\t\treturn (\n",
        "\t\t\tself.word2index[word]\n",
        "\t\t\tif word in self.word2index\n",
        "\t\t\telse self.word2index[UNK_TOKEN]\n",
        "\t\t)\n",
        "\n",
        "\tdef get_word(self, index: int) -> str:\n",
        "\t\t\"\"\"Returns the word at a given index in our vocabulary.\n",
        "        Args:\n",
        "            index: the word position in our vocabulary.\n",
        "        Returns:\n",
        "            The word corresponding to the given index.\n",
        "        \"\"\"\n",
        "\t\treturn self.index2word[index]\n",
        "\n",
        "\tdef get_word_count(self, word: str) -> int:\n",
        "\t\t\"\"\"Returns the number of occurences for a given word.\n",
        "        Raise a \n",
        "        Args:\n",
        "            The word.\n",
        "        Returns:\n",
        "            Its number of measured occurences.\n",
        "        \"\"\"\n",
        "\t\treturn self.word2count[word]\n",
        "\n",
        "\tdef get_vocabulary(self) -> List[str]:\n",
        "\t\t\"\"\"Returns a copy of the whole vocabulary list.\n",
        "        Returns:\n",
        "            A list of words.\n",
        "        \"\"\"\n",
        "\t\treturn deepcopy(self.index2word)\n",
        "\n",
        "\tdef __len__(self) -> int:\n",
        "\t\t\"\"\"len() function.\n",
        "        Returns:\n",
        "            The number of words in the vocabulary.\n",
        "        \"\"\"\n",
        "\t\treturn len(self.index2word)\n",
        "\n",
        "\tdef trim_vocabulary(self, min_occurences: int = 5) -> None:\n",
        "\t\t\"\"\"Trim the vocabulary based on the number of occurrences of each words.\n",
        "        Note that whole counts of deleted words are added to the UNK_TOKEN counts.\n",
        "        Args:\n",
        "            min_occurences: the minimum number of occurences for a word to be kept.\n",
        "        \"\"\"\n",
        "\t\tto_delete = {\n",
        "\t\t\tword for word, count in self.word2count.items() if count < min_occurences\n",
        "\t\t}\n",
        "\t\tnew_word2count = defaultdict(int)\n",
        "\t\tfor word, count in self.word2count.items():\n",
        "\t\t\tif word not in to_delete:\n",
        "\t\t\t\tnew_word2count[word] = count\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_word2count[UNK_TOKEN] += count\n",
        "\t\tnew_index2word = [word for word in self.index2word if word not in to_delete]\n",
        "\t\tnew_word2index = {word: index for index, word in enumerate(new_index2word)}\n",
        "\n",
        "\t\tself.word2count = new_word2count\n",
        "\t\tself.index2word = new_index2word\n",
        "\t\tself.word2index = new_word2index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4rckxAgWTaQ"
      },
      "source": [
        "**(1 point)** Get the vocabulary on both the training and validation set using the `Vocabulary` class. Remember, we don't use the test set here as we consider it as proxy production data. The trim it down as you see fit (around 20K words in the vocabulary is a good value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8eqcMYzfLJr"
      },
      "outputs": [],
      "source": [
        "vocabulary = Vocabulary()\n",
        "for rev in clean_dataset['train']['text']:\n",
        "  vocabulary.add_text(rev)\n",
        "for rev in clean_dataset['validation']['text']:\n",
        "  vocabulary.add_text(rev)\n",
        "# Your code...\n",
        "vocabulary.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.trim_vocabulary(15)\n",
        "vocabulary.__len__()"
      ],
      "metadata": {
        "id": "ykGZb4-kaI8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGLAUDPRWTaS"
      },
      "source": [
        "**(1 point)** Fill the encoding and decoding functions. The encoding function takes a text as input and returns a list IDs corresponding to the index of each word in the vocabulary. The decoding function reverse the process, turning a list of IDs into a text. Make sure the encoding function returns a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mj4AMqYk6xt"
      },
      "outputs": [],
      "source": [
        "# Encoding and decoding function\n",
        "\n",
        "def encode_text(text: str) -> np.ndarray:\n",
        "  tokens = text.split()\n",
        "  return np.array(list(map(lambda word: vocabulary.get_index(word), tokens)))\n",
        "\n",
        "def decode_text(encoded_text: np.ndarray) -> str:\n",
        "  tokens = list(map(lambda index: vocabulary.get_word(index), encoded_text))\n",
        "  return \" \".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPUej2whwaXw"
      },
      "source": [
        "To make sure everything went well, we compare a text before and after encoding and then decoding it. You should see rare words / typos replaced by the `<UNK>` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdq011plYUHG"
      },
      "outputs": [],
      "source": [
        "# Apply the encoding function to the entire dataset.\n",
        "encoded_dataset = clean_dataset.map(lambda x: {\"text\": encode_text(x[\"text\"]), \"label\": x[\"label\"]})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(encoded_dataset[\"train\"][\"text\"][0][0])"
      ],
      "metadata": {
        "id": "NHrra3r4OlyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT8UQgeWliQz"
      },
      "outputs": [],
      "source": [
        "clean_dataset[\"train\"][\"text\"][0], decode_text(encoded_dataset[\"train\"][\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVadYQD2nw52"
      },
      "source": [
        "## Batch preparation **(1 point)**\n",
        "\n",
        "To speed up learning, and take advantage of the GPU architecture, we provide data to the model by batches. Since all line in the same batch need to have the same length, we pad lines to the maximum length of each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWV2pzgqa1cD"
      },
      "outputs": [],
      "source": [
        "def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int = 32, pad_right: bool = False) -> Tuple[\n",
        "\tnp.ndarray, np.ndarray]:\n",
        "\t\"\"\"Generate randomly ordered batches of data+labels.\n",
        "    Args:\n",
        "        X: the input data.\n",
        "        y: the corresponding labels.\n",
        "        batch_size: the size of each batch [32].\n",
        "        pad_right: if true, the padding is done on the right [False].\n",
        "    \"\"\"\n",
        "\n",
        "\tX, y = shuffle(X, y)\n",
        "\tn_batches = int(np.ceil(len(y) / batch_size))\n",
        "\n",
        "\tfor i in range(n_batches):\n",
        "\n",
        "\t\tend = min((i + 1) * batch_size, len(y))\n",
        "\n",
        "\t\tX_batch = X[i * batch_size:end]\n",
        "\t\ty_batch = y[i * batch_size:end]\n",
        "\n",
        "\t\t# Padding to max ength size within the batch\n",
        "\t\tmax_len = np.max([len(x) for x in X_batch])\n",
        "\t\tfor j in range(len(X_batch)):\n",
        "\t\t\tx = X_batch[j]\n",
        "\t\t\tpad = [vocabulary.get_index(PAD_TOKEN)] * (max_len - len(x))\n",
        "\t\t\tX_batch[j] = x + pad if pad_right else pad + x\n",
        "\n",
        "\t\tX_batch = torch.from_numpy(np.array(X_batch)).long()\n",
        "\t\ty_batch = torch.from_numpy(np.array(y_batch)).long()\n",
        "\n",
        "\t\t# Yielding results, so every time the function is called, it starts again from here.\n",
        "\t\tyield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK8ZcGK2WTac"
      },
      "source": [
        "Let's see what the batches look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1-D1ueJb6IR"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"]):\n",
        "\tprint(\"inputs\", inputs, \"shape:\", inputs.shape)\n",
        "\tprint(\"labels\", labels, \"shape:\", labels.shape)\n",
        "\tbreak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idylIiC1WTao"
      },
      "source": [
        "**(1 point)** Question: On which side should we pad the data for our use case and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5uO-cAWodA0"
      },
      "source": [
        "## The model **(13 points)**\n",
        "\n",
        "We use a simple RNN with a configurable number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xel73Svtcgrf"
      },
      "outputs": [],
      "source": [
        "# Before starting, let's set up the device. A GPU if available, else the CPU.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiNVffK_cnxM"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "\t\"\"\"A simple RNN module with word embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n",
        "\t\t\"\"\"\n",
        "        Args:\n",
        "            vocab_size: vocabulary size.\n",
        "            embed_size: embedding dimensions.\n",
        "            hidden_size: hidden layer size.\n",
        "            n_layers: the number of layers.\n",
        "            n_outputs: the number of output classes.\n",
        "        \"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embed_size = embed_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.n_layers = n_layers\n",
        "\t\tself.n_outputs = n_outputs\n",
        "\n",
        "\t\t# The word embedding layer.\n",
        "\t\tself.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "\t\t# The RNN\n",
        "\t\tself.rnn = nn.RNN(\n",
        "\t\t\tinput_size=self.embed_size,\n",
        "\t\t\thidden_size=self.hidden_size,\n",
        "\t\t\tnum_layers=self.n_layers,\n",
        "\t\t\tbatch_first=True,  # Changes the order of dimension to put the batches first.\n",
        "\t\t)\n",
        "\t\t# A fully connected layer to project the RNN's output to only one output used for classification.\n",
        "\t\tself.fc = nn.Linear(self.hidden_size, self.n_outputs)\n",
        "\n",
        "\tdef forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "\t\t\"\"\"Function called when the model is called with data as input.\n",
        "        Args:\n",
        "            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n",
        "        Returns:\n",
        "            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n",
        "        \"\"\"\n",
        "\t\th0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n",
        "\n",
        "\t\tout = self.embed(X)\n",
        "\t\t# out contains the output layer of all words in the sequence.\n",
        "\t\t# First dim is batch, second the word in the sequence, third is the vector itself.\n",
        "\t\t# The second output value is the last vector of all intermediate layer.\n",
        "\t\t# Only use it if you want to access the intermediate layer values of a\n",
        "\t\t# multilayer model.\n",
        "\t\tout, _ = self.rnn(out, h0)\n",
        "\t\t# Getting the last value only.\n",
        "\t\tout = out[:, -1, :]\n",
        "\n",
        "\t\t# Linear projection.\n",
        "\t\tout = self.fc(out)\n",
        "\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyAl4NEDWTat"
      },
      "source": [
        "Note that we do not pass the output through a sigmoid function. This is because pyTorch implements some code optimization within the `BCEWithLogitsLoss` we'll see later."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_accuracy(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    outputs = torch.sigmoid(outputs)\n",
        "    outputs = torch.where(outputs > 0.5, 1, 0)\n",
        "    acc = (torch.sum(outputs == labels).float()/labels.size()[0]).item()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "f_A6P4j3H7ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw-lzgfveXB3"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def train(\n",
        "\t\tmodel: nn.Module,\n",
        "\t\tcriterion: Callable,\n",
        "\t\toptimizier: torch.optim.Optimizer,\n",
        "\t\tn_epochs: int,\n",
        "\t\ttrain_gen: Callable,\n",
        "\t\tvalid_gen: Callable,\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "\t\"\"\"Train a model using a batch gradient descent.\n",
        "    Args:\n",
        "        model: a class inheriting from nn.Module.\n",
        "        criterion: a loss criterion.\n",
        "        optimizer: an optimizer (e.g. Adam, RMSprop, ...).\n",
        "        n_epochs: the number of training epochs.\n",
        "        train_gen: a callable function returing a batch (data, labels).\n",
        "        valid_gen: a callable function returing a batch (data, labels).\n",
        "    \"\"\"\n",
        "\ttrain_losses = np.zeros(n_epochs)\n",
        "\tvalid_losses = np.zeros(n_epochs)\n",
        "\tbest_val_loss = float('inf')\n",
        "\tbest_model = deepcopy(model)\n",
        "\ttrain_accuracies = np.zeros(n_epochs)\n",
        "\tvalid_accuracies = np.zeros(n_epochs)\n",
        "\n",
        "\tfor epoch in range(n_epochs):\n",
        "\n",
        "\t\tt0 = datetime.now()\n",
        "\t\tmodel.train()\n",
        "\t\ttrain_loss = []\n",
        "\t\ttrain_accuracy = []\n",
        "\n",
        "\t\t# Training loop.\n",
        "\t\tfor inputs, labels in train_gen():\n",
        "\t\t\t# labels are of dimension (N,) we turn them into (N, 1).\n",
        "\t\t\tlabels = labels.view(-1, 1).float()\n",
        "\t\t\t# Put them on the GPU.\n",
        "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "\t\t\t# Reset the gradient.\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\toutputs = model(inputs)\n",
        "\t\t\ttrain_accuracy.append(my_accuracy(outputs, labels))\n",
        "\t\t\tloss = criterion(outputs, labels)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizier.step()\n",
        "\t\t\t\n",
        "\t\t\ttrain_loss.append(loss.item())  # .item() detach the value from GPU.\n",
        "   \n",
        "\t\ttrain_losses[epoch] = np.mean(train_loss)\n",
        "\t\ttrain_accuracies[epoch] = np.mean(train_accuracy)\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tvalid_loss = []\n",
        "\t\tvalid_accuracy = []\n",
        "\t\t# Evaluation loop.\n",
        "\t\tfor inputs, labels in valid_gen():\n",
        "\t\t\tlabels = labels.view(-1, 1).float()\n",
        "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "\t\t\toutputs = model(inputs)\n",
        "\t\t\tmy_accuracy(outputs, labels)\n",
        "\t\t\tvalid_accuracy.append(my_accuracy(outputs, labels))\n",
        "\t\t\tloss = criterion(outputs, labels)\n",
        "\n",
        "\t\t\tvalid_loss.append(loss.item())\n",
        "\n",
        "\t\tvalid_losses[epoch] = np.mean(valid_loss)\n",
        "\t\tvalid_accuracies[epoch] = np.mean(valid_accuracy)\n",
        "  \n",
        "\t\tif valid_losses[epoch] < best_val_loss:\n",
        "\t\t\tbest_val_loss = valid_losses[epoch]\n",
        "\t\t\tbest_model = deepcopy(model)\n",
        "\t\tprint(\n",
        "\t\t\tf\"Epoch: {epoch}, training loss: {train_losses[epoch]}, validation loss: {valid_losses[epoch]}, training accuracy: {train_accuracies[epoch]}, validation accuracy: {valid_accuracies[epoch]}, in {datetime.now() - t0}\")\n",
        "\treturn train_losses, valid_losses, best_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCfXsLZUWTav"
      },
      "source": [
        "We setup the model, criterion (a binary cross entropy), and the optimizer (Adam).\n",
        "\n",
        "Note that `BCEWithLogitsLoss` use a mathematical trick to incorporate the sigmoid function in its computation. This trick makes the learning process go slightly faster and is the reason why we didn't put a sigmoid in the forward function of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Mpe44Dd85Z"
      },
      "outputs": [],
      "source": [
        "model = RNN(len(vocabulary), 32, 64, 1, 1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeJn4q3bWTaz"
      },
      "source": [
        "We get the 3 generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srYPmX_aeLwD"
      },
      "outputs": [],
      "source": [
        "train_gen = lambda: data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"])\n",
        "valid_gen = lambda: data_generator(encoded_dataset[\"validation\"][\"text\"], encoded_dataset[\"validation\"][\"label\"])\n",
        "test_gen = lambda: data_generator(encoded_dataset[\"test\"][\"text\"], encoded_dataset[\"test\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJhK3MUAWTa1"
      },
      "source": [
        "And train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB98HqD3fk1G"
      },
      "outputs": [],
      "source": [
        "train_losses, valid_losses, model = train(model, criterion, optimizer, 20, train_gen, valid_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwZAUXrnWTa2"
      },
      "source": [
        "We can look at the training and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VATw05GYfwNm"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label=\"Training loss\")\n",
        "plt.plot(valid_losses, label=\"Validation loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SfRJZTBWTa4"
      },
      "source": [
        "For the assignment, code the following.\n",
        "* **(2 points)** The model validation loss should go down and then up. It means the model starts overfitting after a certain number of iterations. Modify the `train` function so it returns the model found with the best validation loss.\n",
        "* **(2 points)** Add an accuracy function and report the accuracy of the training and test set.\n",
        "* **(3 points)** Create an LSTM class which uses an LSTM instead of an RNN. Compare its results with the RNN.\n",
        "* **(2 point)** Implement a function which takes any text and return the model's prediction.\n",
        "    * The function should have a string as input and return a class (0 or 1) and its probability (score out of a [sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html)).\n",
        "    * Don't forget to make the text go through the same pretreatment and encoding you used to train your model.\n",
        "* **(3 points)** Create a bidirectional LSTM (BiLSTM) class to classify your sentences. Report the accuracy on the training and test data.\n",
        "    * To combine the last output of both direction, you can concatenate, add, or max-pool them. Please document your choice.\n",
        "* **(1 point)** With your best classifier, look at two wrongly classified examples on the test set. Try explaining why the model was wrong.\n",
        "* **(Bonus)** Try finding better hyperparameters (dimensions, number of layers, ...). Document your experiments and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5FZbgwK-yoX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}