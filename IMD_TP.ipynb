{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DlzvXLQuoTY"
      },
      "source": [
        "# Sentiment classifier with LSTM\n",
        "\n",
        "In this notebook, we will implement a simple sentiment classifier using an LSTM. Follow the notebook, complete the missing part, answer the questions and apply the asked modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utTvPhcyTC7d",
        "outputId": "ff7e71bb-ea0c-4fca-96fe-eca4f8ebbb1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.12.0)\n",
            "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "g_v62rH_W9J1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import re\n",
        "from typing import Callable, List, Tuple\n",
        "\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.auto import tqdm\n",
        "from datetime import datetime\n",
        "from sklearn.utils import shuffle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzAHrPfReDqu"
      },
      "source": [
        "## Downloading the dataset\n",
        "\n",
        "Using the datasets library, we load the imdb dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "0033e0bd7d334fbe8ccc89c55e45a95e",
            "9032c7332c9a4a94ab7d5eef9fd3659e",
            "fb87b0b040864ec4b2a4d6321bd48924",
            "0fce7e7b04344b23b7087fb7e168c8d3",
            "345116536b434ff0b873da7f2c10edad",
            "fb171a37591840ec9650f94d83f93c95",
            "7c10b01485934c95a83beba14bc40901",
            "36c6e5ab1e844743ac4c94c59eb07acd",
            "a9e3e74ab6d64ec384a9239843e77712",
            "0412694b06224d41a11d3c9a636334c7",
            "b03184e9b6a9422baec681714220b8e4"
          ]
        },
        "id": "PBb40sTOXYz-",
        "outputId": "fcd513bf-3779-459f-b919-cc5a8bec973f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_deprecation.py:97: FutureWarning: Deprecated argument(s) used in 'dataset_info': token. Will not be supported from version '0.12'.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "WARNING:datasets.builder:Found cached dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0033e0bd7d334fbe8ccc89c55e45a95e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDq8WhU3XlOR",
        "outputId": "c22ba27d-6268-4244-ad95-078c2662d14c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kr3Qby4KWTZ6",
        "outputId": "25471139-94a4-411c-dbb7-3d30c56fd609"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'label'],\n",
              "    num_rows: 50000\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# We do not need the \"unsupervised\" split.\n",
        "dataset.pop(\"unsupervised\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47Hy5byjWTZ8",
        "outputId": "7437f7d5-f1b8-48e7-db06-67bb968fdcb6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXssc1MjX5v2",
        "outputId": "083a31ca-f294-488a-b904-b0d2e2eb2746"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
              "  '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.'],\n",
              " 'label': [0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "dataset[\"train\"][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UoMzhBOeGxe"
      },
      "source": [
        "## Pretreatment **(1 point)**\n",
        "\n",
        "Code the `pretreatment` function which clean the input text. Look at the dataset and deduce which treatment is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Cw3lht4AZHQB"
      },
      "outputs": [],
      "source": [
        "from string import punctuation\n",
        "\n",
        "\n",
        "def pretreatment(text: str) -> str:\n",
        "\t\"\"\"Clean IMDB text entries.\n",
        "    Args:\n",
        "        text: an input string.\n",
        "    Returns:\n",
        "        The cleaned text.\n",
        "    \"\"\"\n",
        "\t# Your code here\n",
        "\tlower = text.lower()\n",
        "\treturn lower.translate(str.maketrans('', '', punctuation))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnxsNRj1b4ef",
        "outputId": "6ef4a7d6-34da-4c83-e2b1-db6563f5c22f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-c114ae7ed685fd67.arrow\n",
            "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-8a0abe44e0435fb5.arrow\n"
          ]
        }
      ],
      "source": [
        "# This applies the pretreatment function to all\n",
        "clean_dataset = dataset.map(lambda x: {\"text\": pretreatment(x[\"text\"]), \"label\": x[\"label\"]})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cITYcafjvZ4V"
      },
      "source": [
        "Let's see what the text now look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss9fNfoWUAz_",
        "outputId": "379837f6-723b-44c2-c63a-fee2a6ebb0de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i rented i am curiousyellow from my video store because of all the controversy that surrounded it when it was first released in 1967 i also heard that at first it was seized by us customs if it ever tried to enter this country therefore being a fan of films considered controversial i really had to see this for myselfbr br the plot is centered around a young swedish drama student named lena who wants to learn everything she can about life in particular she wants to focus her attentions to making some sort of documentary on what the average swede thought about certain political issues such as the vietnam war and race issues in the united states in between asking politicians and ordinary denizens of stockholm about their opinions on politics she has sex with her drama teacher classmates and married menbr br what kills me about i am curiousyellow is that 40 years ago this was considered pornographic really the sex and nudity scenes are few and far between even then its not shot like some cheaply made porno while my countrymen mind find it shocking in reality sex and nudity are a major staple in swedish cinema even ingmar bergman arguably their answer to good old boy john ford had sex scenes in his filmsbr br i do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in america i am curiousyellow is a good film for anyone wanting to study the meat and potatoes no pun intended of swedish cinema but really this film doesnt have much of a plot',\n",
              " 'i am curious yellow is a risible and pretentious steaming pile it doesnt matter what ones political views are because this film can hardly be taken seriously on any level as for the claim that frontal male nudity is an automatic nc17 that isnt true ive seen rrated films with male nudity granted they only offer some fleeting views but where are the rrated films with gaping vulvas and flapping labia nowhere because they dont exist the same goes for those crappy cable shows schlongs swinging in the breeze but not a clitoris in sight and those pretentious indie movies like the brown bunny in which were treated to the site of vincent gallos throbbing johnson but not a trace of pink visible on chloe sevigny before crying or implying doublestandard in matters of nudity the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women there are no genitals on display when actresses appears nude and the same cannot be said for a man in fact you generally wont see female genitals in an american film in anything short of porn or explicit erotica this alleged doublestandard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of womens bodies',\n",
              " 'if only to avoid making this type of film in the future this film is interesting as an experiment but tells no cogent storybr br one might feel virtuous for sitting thru it because it touches on so many important issues but it does so without any discernable motive the viewer comes away with no new perspectives unless one comes up with one while ones mind wanders as it will invariably do during this pointless filmbr br one might better spend ones time staring out a window at a tree growingbr br ']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "clean_dataset[\"train\"][\"text\"][:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sAscTs3U2Sn"
      },
      "source": [
        "Let's take a quick look at the labels. Notice that the labels are ordered in the training set starting by the negative reviews (0), followed by the positive ones (1). Training neural networks on this kind of configuration tends to considerably affect their performances. So the dataset will have to be shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZedAEQpdstA",
        "outputId": "2c9de7f2-3f1c-49aa-890c-e9de5c9a7b64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "clean_dataset[\"train\"][\"label\"][12490:12510]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv3rt4ZieQbN"
      },
      "source": [
        "## Train/validation split **(1 point)**\n",
        "\n",
        "In our example, we consider the test split as production data. Which means, we need to treat it as if we never see it during the training process. To experiment on the model, we need to split the training set into a training and validation set. See [here](https://huggingface.co/course/chapter5/3?fw=pt#creating-a-validation-set) on how to do so with the `Datasets` library.\n",
        "\n",
        "Don't forget to **stratify** your split (we need to have the same proportion of class in both training and validation set)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5QIBkz-0YAO7"
      },
      "outputs": [],
      "source": [
        "l = int(25000/2)\n",
        "perc = 0.80\n",
        "lperc= int(l*perc)\n",
        "\n",
        "train_text =clean_dataset['train']['text'][:lperc] +  clean_dataset['train']['text'][l:l+lperc]\n",
        "val_text = clean_dataset['train']['text'][lperc:l] +  clean_dataset['train']['text'][l+lperc:2*l]\n",
        "\n",
        "train_label=clean_dataset['train']['label'][:lperc] +  clean_dataset['train']['label'][l:l+lperc]\n",
        "val_label= clean_dataset['train']['label'][lperc:l] +  clean_dataset['train']['label'][l+lperc:2*l]\n",
        "\n",
        "train = {'text': train_text, 'label': train_label}\n",
        "val = {'text': val_text, 'label': val_label}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from  datasets import Dataset\n",
        "dataset2 = Dataset({'train': train, 'val': val})"
      ],
      "metadata": {
        "id": "h3gqwi1hXhlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "# X_train,X_val,Y_train,Y_val = train_test_split(clean_dataset[\"train\"][\"text\"],clean_dataset[\"train\"][\"label\"],test_size=0.2,shuffle=True)"
      ],
      "metadata": {
        "id": "K5L5UYHlWTaJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# sum(Y_train)\n"
      ],
      "metadata": {
        "id": "TJPgOxLiWTaK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "F58TBVUbVfWb",
        "outputId": "b7d87bbf-ab80-4ab0-fe98-517854461383"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-fb0b9d232b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset2' is not defined"
          ]
        }
      ],
      "source": [
        "dataset2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w97hMnYEeexj"
      },
      "outputs": [],
      "source": [
        "# Check here that the dataset is stratified."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0t40J_xte1dz"
      },
      "source": [
        "## Categorical encoding of the vocabulary **(2 points)**\n",
        "\n",
        "We can't feed word to a neural network. A usual solution is to turn words into categorical data by using one-hot encoding. To avoid an explosion in vocabulary size, we will only keep words which appear more than a certain amount of time.\n",
        "\n",
        "The `Vocabulary` class below will do that for us."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2P9A4j8aarj"
      },
      "outputs": [],
      "source": [
        "UNK_TOKEN = \"<UNK>\"\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "\t\"\"\"Vocabulary manager on a collection.\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self) -> None:\n",
        "\t\t\"\"\"No parameters to provide.\n",
        "        \"\"\"\n",
        "\t\t# Index to word mapping.\n",
        "\t\tself.index2word = [PAD_TOKEN, UNK_TOKEN]\n",
        "\t\t# Word to index mapping.\n",
        "\t\tself.word2index = {value: key for key, value in enumerate(self.index2word)}\n",
        "\t\t# Word counter.\n",
        "\t\tself.word2count = defaultdict(int)\n",
        "\n",
        "\tdef add_word(self, word: str) -> None:\n",
        "\t\t\"\"\"Increments the count of a word to the vocabulary.\n",
        "        Args:\n",
        "            word: the word.\n",
        "        \"\"\"\n",
        "\t\tself.word2count[word] += 1\n",
        "\t\tif not word in self.word2index:\n",
        "\t\t\tself.word2index[word] = len(self.index2word)\n",
        "\t\t\tself.index2word.append(word)\n",
        "\n",
        "\tdef add_text(self, text: str, separator: str = \" \") -> None:\n",
        "\t\t\"\"\"Add the words given in a text to our vocabulary.\n",
        "        Args:\n",
        "            text: a sequence of words separated by a given separator.\n",
        "            separator: the separator used to split our text (default is \" \").\n",
        "        \"\"\"\n",
        "\t\tfor word in text.split(separator):\n",
        "\t\t\tself.add_word(word)\n",
        "\n",
        "\tdef get_index(self, word: str) -> int:\n",
        "\t\t\"\"\"Returns the index of a given word in our vocabulary.\n",
        "        If the word is not in the vocabulary, returns the index for UNK_TOKEN.\n",
        "        Args:\n",
        "            word: a string.\n",
        "        Returns:\n",
        "            The corresponding index or the index for UNK_TOKEN.\n",
        "        \"\"\"\n",
        "\t\treturn (\n",
        "\t\t\tself.word2index[word]\n",
        "\t\t\tif word in self.word2index\n",
        "\t\t\telse self.word2index[UNK_TOKEN]\n",
        "\t\t)\n",
        "\n",
        "\tdef get_word(self, index: int) -> str:\n",
        "\t\t\"\"\"Returns the word at a given index in our vocabulary.\n",
        "        Args:\n",
        "            index: the word position in our vocabulary.\n",
        "        Returns:\n",
        "            The word corresponding to the given index.\n",
        "        \"\"\"\n",
        "\t\treturn self.index2word[index]\n",
        "\n",
        "\tdef get_word_count(self, word: str) -> int:\n",
        "\t\t\"\"\"Returns the number of occurences for a given word.\n",
        "        Raise a \n",
        "        Args:\n",
        "            The word.\n",
        "        Returns:\n",
        "            Its number of measured occurences.\n",
        "        \"\"\"\n",
        "\t\treturn self.word2count[word]\n",
        "\n",
        "\tdef get_vocabulary(self) -> List[str]:\n",
        "\t\t\"\"\"Returns a copy of the whole vocabulary list.\n",
        "        Returns:\n",
        "            A list of words.\n",
        "        \"\"\"\n",
        "\t\treturn deepcopy(self.index2word)\n",
        "\n",
        "\tdef __len__(self) -> int:\n",
        "\t\t\"\"\"len() function.\n",
        "        Returns:\n",
        "            The number of words in the vocabulary.\n",
        "        \"\"\"\n",
        "\t\treturn len(self.index2word)\n",
        "\n",
        "\tdef trim_vocabulary(self, min_occurences: int = 5) -> None:\n",
        "\t\t\"\"\"Trim the vocabulary based on the number of occurrences of each words.\n",
        "        Note that whole counts of deleted words are added to the UNK_TOKEN counts.\n",
        "        Args:\n",
        "            min_occurences: the minimum number of occurences for a word to be kept.\n",
        "        \"\"\"\n",
        "\t\tto_delete = {\n",
        "\t\t\tword for word, count in self.word2count.items() if count < min_occurences\n",
        "\t\t}\n",
        "\t\tnew_word2count = defaultdict(int)\n",
        "\t\tfor word, count in self.word2count.items():\n",
        "\t\t\tif word not in to_delete:\n",
        "\t\t\t\tnew_word2count[word] = count\n",
        "\t\t\telse:\n",
        "\t\t\t\tnew_word2count[UNK_TOKEN] += count\n",
        "\t\tnew_index2word = [word for word in self.index2word if word not in to_delete]\n",
        "\t\tnew_word2index = {word: index for index, word in enumerate(new_index2word)}\n",
        "\n",
        "\t\tself.word2count = new_word2count\n",
        "\t\tself.index2word = new_index2word\n",
        "\t\tself.word2index = new_word2index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4rckxAgWTaQ"
      },
      "source": [
        "**(1 point)** Get the vocabulary on both the training and validation set using the `Vocabulary` class. Remember, we don't use the test set here as we consider it as proxy production data. The trim it down as you see fit (around 20K words in the vocabulary is a good value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8eqcMYzfLJr"
      },
      "outputs": [],
      "source": [
        "vocabulary = Vocabulary()\n",
        "for rev in clean_dataset['train']['text']:\n",
        "  vocabulary.add_text(rev)\n",
        "for rev in clean_dataset['val']['text']:\n",
        "  vocabulary.add_text(rev)\n",
        "# Your code...\n",
        "vocabulary.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary.trim_vocabulary(10)\n",
        "vocabulary.__len__()"
      ],
      "metadata": {
        "id": "ykGZb4-kaI8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGLAUDPRWTaS"
      },
      "source": [
        "**(1 point)** Fill the encoding and decoding functions. The encoding function takes a text as input and returns a list IDs corresponding to the index of each word in the vocabulary. The decoding function reverse the process, turning a list of IDs into a text. Make sure the encoding function returns a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Mj4AMqYk6xt"
      },
      "outputs": [],
      "source": [
        "# Encoding and decoding function\n",
        "\n",
        "def encode_text(text: str) -> np.ndarray:\n",
        "  tokens = text.split()\n",
        "  np.array(map(lambda word: vocabulary.get_index(word) for word in tokens))\n",
        "  return tokens\n",
        "\n",
        "def decode_text(encoded_text: np.ndarray) -> str:\n",
        "  tokens = list(map(lambda index: vocabulary.get_word(index) for index in encoded_text))\n",
        "  return \"\".join(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPUej2whwaXw"
      },
      "source": [
        "To make sure everything went well, we compare a text before and after encoding and then decoding it. You should see rare words / typos replaced by the `<UNK>` token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdq011plYUHG"
      },
      "outputs": [],
      "source": [
        "# Apply the encoding function to the entire dataset.\n",
        "encoded_dataset = clean_dataset.map(lambda x: {\"text\": encode_text(x[\"text\"]), \"label\": x[\"label\"]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RT8UQgeWliQz"
      },
      "outputs": [],
      "source": [
        "clean_dataset[\"train\"][\"text\"][0], decode_text(encoded_dataset[\"train\"][\"text\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVadYQD2nw52"
      },
      "source": [
        "## Batch preparation **(1 point)**\n",
        "\n",
        "To speed up learning, and take advantage of the GPU architecture, we provide data to the model by batches. Since all line in the same batch need to have the same length, we pad lines to the maximum length of each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWV2pzgqa1cD"
      },
      "outputs": [],
      "source": [
        "def data_generator(X: np.ndarray, y: np.ndarray, batch_size: int = 32, pad_right: bool = False) -> Tuple[\n",
        "\tnp.ndarray, np.ndarray]:\n",
        "\t\"\"\"Generate randomly ordered batches of data+labels.\n",
        "    Args:\n",
        "        X: the input data.\n",
        "        y: the corresponding labels.\n",
        "        batch_size: the size of each batch [32].\n",
        "        pad_right: if true, the padding is done on the right [False].\n",
        "    \"\"\"\n",
        "\n",
        "\tX, y = shuffle(X, y)\n",
        "\tn_batches = int(np.ceil(len(y) / batch_size))\n",
        "\n",
        "\tfor i in range(n_batches):\n",
        "\n",
        "\t\tend = min((i + 1) * batch_size, len(y))\n",
        "\n",
        "\t\tX_batch = X[i * batch_size:end]\n",
        "\t\ty_batch = y[i * batch_size:end]\n",
        "\n",
        "\t\t# Padding to max ength size within the batch\n",
        "\t\tmax_len = np.max([len(x) for x in X_batch])\n",
        "\t\tfor j in range(len(X_batch)):\n",
        "\t\t\tx = X_batch[j]\n",
        "\t\t\tpad = [vocabulary.get_index(PAD_TOKEN)] * (max_len - len(x))\n",
        "\t\t\tX_batch[j] = x + pad if pad_right else pad + x\n",
        "\n",
        "\t\tX_batch = torch.from_numpy(np.array(X_batch)).long()\n",
        "\t\ty_batch = torch.from_numpy(np.array(y_batch)).long()\n",
        "\n",
        "\t\t# Yielding results, so every time the function is called, it starts again from here.\n",
        "\t\tyield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kK8ZcGK2WTac"
      },
      "source": [
        "Let's see what the batches look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1-D1ueJb6IR"
      },
      "outputs": [],
      "source": [
        "for inputs, labels in data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"]):\n",
        "\tprint(\"inputs\", inputs, \"shape:\", inputs.shape)\n",
        "\tprint(\"labels\", labels, \"shape:\", labels.shape)\n",
        "\tbreak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idylIiC1WTao"
      },
      "source": [
        "**(1 point)** Question: On which side should we pad the data for our use case and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5uO-cAWodA0"
      },
      "source": [
        "## The model **(13 points)**\n",
        "\n",
        "We use a simple RNN with a configurable number of layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xel73Svtcgrf"
      },
      "outputs": [],
      "source": [
        "# Before starting, let's set up the device. A GPU if available, else the CPU.\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiNVffK_cnxM"
      },
      "outputs": [],
      "source": [
        "class RNN(nn.Module):\n",
        "\t\"\"\"A simple RNN module with word embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "\tdef __init__(self, vocab_size: int, embed_size: int, hidden_size: int, n_layers: int, n_outputs: int) -> None:\n",
        "\t\t\"\"\"\n",
        "        Args:\n",
        "            vocab_size: vocabulary size.\n",
        "            embed_size: embedding dimensions.\n",
        "            hidden_size: hidden layer size.\n",
        "            n_layers: the number of layers.\n",
        "            n_outputs: the number of output classes.\n",
        "        \"\"\"\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embed_size = embed_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.n_layers = n_layers\n",
        "\t\tself.n_outputs = n_outputs\n",
        "\n",
        "\t\t# The word embedding layer.\n",
        "\t\tself.embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "\t\t# The RNN\n",
        "\t\tself.rnn = nn.RNN(\n",
        "\t\t\tinput_size=self.embed_size,\n",
        "\t\t\thidden_size=self.hidden_size,\n",
        "\t\t\tnum_layers=self.n_layers,\n",
        "\t\t\tbatch_first=True,  # Changes the order of dimension to put the batches first.\n",
        "\t\t)\n",
        "\t\t# A fully connected layer to project the RNN's output to only one output used for classification.\n",
        "\t\tself.fc = nn.Linear(self.hidden_size, self.n_outputs)\n",
        "\n",
        "\tdef forward(self, X: torch.Tensor) -> torch.Tensor:\n",
        "\t\t\"\"\"Function called when the model is called with data as input.\n",
        "        Args:\n",
        "            X: the input tensor of dimensions batch_size, sequence length, vocab size (actually just an int).\n",
        "        Returns:\n",
        "            The resulting tensor of dimension batch_size, sequence length, output dimensions.\n",
        "        \"\"\"\n",
        "\t\th0 = torch.zeros(self.n_layers, X.size(0), self.hidden_size).to(device)\n",
        "\n",
        "\t\tout = self.embed(X)\n",
        "\t\t# out contains the output layer of all words in the sequence.\n",
        "\t\t# First dim is batch, second the word in the sequence, third is the vector itself.\n",
        "\t\t# The second output value is the last vector of all intermediate layer.\n",
        "\t\t# Only use it if you want to access the intermediate layer values of a\n",
        "\t\t# multilayer model.\n",
        "\t\tout, _ = self.rnn(out, h0)\n",
        "\t\t# Getting the last value only.\n",
        "\t\tout = out[:, -1, :]\n",
        "\n",
        "\t\t# Linear projection.\n",
        "\t\tout = self.fc(out)\n",
        "\n",
        "\t\treturn out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyAl4NEDWTat"
      },
      "source": [
        "Note that we do not pass the output through a sigmoid function. This is because pyTorch implements some code optimization within the `BCEWithLogitsLoss` we'll see later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zw-lzgfveXB3"
      },
      "outputs": [],
      "source": [
        "def train(\n",
        "\t\tmodel: nn.Module,\n",
        "\t\tcriterion: Callable,\n",
        "\t\toptimizier: torch.optim.Optimizer,\n",
        "\t\tn_epochs: int,\n",
        "\t\ttrain_gen: Callable,\n",
        "\t\tvalid_gen: Callable,\n",
        ") -> Tuple[List[float], List[float]]:\n",
        "\t\"\"\"Train a model using a batch gradient descent.\n",
        "    Args:\n",
        "        model: a class inheriting from nn.Module.\n",
        "        criterion: a loss criterion.\n",
        "        optimizer: an optimizer (e.g. Adam, RMSprop, ...).\n",
        "        n_epochs: the number of training epochs.\n",
        "        train_gen: a callable function returing a batch (data, labels).\n",
        "        valid_gen: a callable function returing a batch (data, labels).\n",
        "    \"\"\"\n",
        "\ttrain_losses = np.zeros(n_epochs)\n",
        "\tvalid_losses = np.zeros(n_epochs)\n",
        "\n",
        "\tfor epoch in range(n_epochs):\n",
        "\n",
        "\t\tt0 = datetime.now()\n",
        "\t\tmodel.train()\n",
        "\t\ttrain_loss = []\n",
        "\n",
        "\t\t# Training loop.\n",
        "\t\tfor inputs, labels in train_gen():\n",
        "\t\t\t# labels are of dimension (N,) we turn them into (N, 1).\n",
        "\t\t\tlabels = labels.view(-1, 1).float()\n",
        "\t\t\t# Put them on the GPU.\n",
        "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "\t\t\t# Reset the gradient.\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\toutputs = model(inputs)\n",
        "\t\t\tloss = criterion(outputs, labels)\n",
        "\t\t\tloss.backward()\n",
        "\t\t\toptimizier.step()\n",
        "\n",
        "\t\t\ttrain_loss.append(loss.item())  # .item() detach the value from GPU.\n",
        "\n",
        "\t\ttrain_losses[epoch] = np.mean(train_loss)\n",
        "\n",
        "\t\tmodel.eval()\n",
        "\t\tvalid_loss = []\n",
        "\t\t# Evaluation loop.\n",
        "\t\tfor inputs, labels in valid_gen():\n",
        "\t\t\tlabels = labels.view(-1, 1).float()\n",
        "\t\t\tinputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "\t\t\toutputs = model(inputs)\n",
        "\t\t\tloss = criterion(outputs, labels)\n",
        "\n",
        "\t\t\tvalid_loss.append(loss.item())\n",
        "\n",
        "\t\tvalid_losses[epoch] = np.mean(valid_loss)\n",
        "\n",
        "\t\tprint(\n",
        "\t\t\tf\"Epoch: {epoch}, training loss: {train_losses[epoch]}, validation loss: {valid_losses[epoch]}, in {datetime.now() - t0}\")\n",
        "\n",
        "\treturn train_losses, valid_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCfXsLZUWTav"
      },
      "source": [
        "We setup the model, criterion (a binary cross entropy), and the optimizer (Adam).\n",
        "\n",
        "Note that `BCEWithLogitsLoss` use a mathematical trick to incorporate the sigmoid function in its computation. This trick makes the learning process go slightly faster and is the reason why we didn't put a sigmoid in the forward function of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7Mpe44Dd85Z"
      },
      "outputs": [],
      "source": [
        "model = RNN(len(vocabulary), 32, 64, 1, 1).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeJn4q3bWTaz"
      },
      "source": [
        "We get the 3 generators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srYPmX_aeLwD"
      },
      "outputs": [],
      "source": [
        "train_gen = lambda: data_generator(encoded_dataset[\"train\"][\"text\"], encoded_dataset[\"train\"][\"label\"])\n",
        "valid_gen = lambda: data_generator(encoded_dataset[\"validation\"][\"text\"], encoded_dataset[\"validation\"][\"label\"])\n",
        "test_gen = lambda: data_generator(encoded_dataset[\"test\"][\"text\"], encoded_dataset[\"test\"][\"label\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJhK3MUAWTa1"
      },
      "source": [
        "And train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB98HqD3fk1G"
      },
      "outputs": [],
      "source": [
        "train_losses, valid_losses, model = train(model, criterion, optimizer, 20, train_gen, valid_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwZAUXrnWTa2"
      },
      "source": [
        "We can look at the training and validation loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VATw05GYfwNm"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label=\"Training loss\")\n",
        "plt.plot(valid_losses, label=\"Validation loss\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SfRJZTBWTa4"
      },
      "source": [
        "For the assignment, code the following.\n",
        "* **(2 points)** The model validation loss should go down and then up. It means the model starts overfitting after a certain number of iterations. Modify the `train` function so it returns the model found with the best validation loss.\n",
        "* **(2 points)** Add an accuracy function and report the accuracy of the training and test set.\n",
        "* **(3 points)** Create an LSTM class which uses an LSTM instead of an RNN. Compare its results with the RNN.\n",
        "* **(2 point)** Implement a function which takes any text and return the model's prediction.\n",
        "    * The function should have a string as input and return a class (0 or 1) and its probability (score out of a [sigmoid](https://pytorch.org/docs/stable/generated/torch.sigmoid.html)).\n",
        "    * Don't forget to make the text go through the same pretreatment and encoding you used to train your model.\n",
        "* **(3 points)** Create a bidirectional LSTM (BiLSTM) class to classify your sentences. Report the accuracy on the training and test data.\n",
        "    * To combine the last output of both direction, you can concatenate, add, or max-pool them. Please document your choice.\n",
        "* **(1 point)** With your best classifier, look at two wrongly classified examples on the test set. Try explaining why the model was wrong.\n",
        "* **(Bonus)** Try finding better hyperparameters (dimensions, number of layers, ...). Document your experiments and results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e5FZbgwK-yoX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0033e0bd7d334fbe8ccc89c55e45a95e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9032c7332c9a4a94ab7d5eef9fd3659e",
              "IPY_MODEL_fb87b0b040864ec4b2a4d6321bd48924",
              "IPY_MODEL_0fce7e7b04344b23b7087fb7e168c8d3"
            ],
            "layout": "IPY_MODEL_345116536b434ff0b873da7f2c10edad"
          }
        },
        "9032c7332c9a4a94ab7d5eef9fd3659e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb171a37591840ec9650f94d83f93c95",
            "placeholder": "",
            "style": "IPY_MODEL_7c10b01485934c95a83beba14bc40901",
            "value": "100%"
          }
        },
        "fb87b0b040864ec4b2a4d6321bd48924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36c6e5ab1e844743ac4c94c59eb07acd",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9e3e74ab6d64ec384a9239843e77712",
            "value": 3
          }
        },
        "0fce7e7b04344b23b7087fb7e168c8d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0412694b06224d41a11d3c9a636334c7",
            "placeholder": "",
            "style": "IPY_MODEL_b03184e9b6a9422baec681714220b8e4",
            "value": " 3/3 [00:00&lt;00:00,  6.45it/s]"
          }
        },
        "345116536b434ff0b873da7f2c10edad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb171a37591840ec9650f94d83f93c95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c10b01485934c95a83beba14bc40901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36c6e5ab1e844743ac4c94c59eb07acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9e3e74ab6d64ec384a9239843e77712": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0412694b06224d41a11d3c9a636334c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b03184e9b6a9422baec681714220b8e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}