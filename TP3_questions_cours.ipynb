{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "* **(2 points)** Explain with your own words, using a short paragraph for each, what are:\n",
        "        Phonetics and phonology\n",
        "        Morphology and syntax\n",
        "        Semantics and pragmatics"
      ],
      "metadata": {
        "id": "S1sIKWIfKtjG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-*Phonology* is the study of how words sound, (or sign for language with no sound, for example sign language)\n",
        "\n",
        "-*Morphology* is the study about the construction of words, and the relationship of each word to each other. (For example words that stem from the same radical).\n",
        "\n",
        "-*Semantics* is the study of meanings. It applies both to words and sentences. It establishes the bridge between mental ideas and their linguistic representation."
      ],
      "metadata": {
        "id": "uLmxGQyPKvDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(1 point)** What is the difference between stemming and lemmatization?\n",
        "        How do they both work?\n",
        "        What are the pros and cons of both methods ?\n",
        "- Both methods aims to reduce the inflectional forms of each words into a common base of root. They differ on some points, namely:\n",
        "-  *Stemming* cut the affixes (beginning and end) of a word.\n",
        "- *Lemmatization* keep only the root of the words. ()\n",
        "\n",
        "The goal is to reduce the number of word that we have to put in our model as well as the volume of our vocabulary.\n",
        "\n",
        "The advantage with lemmas is that it can have the root of a word even if it is radically changed. (Example: the lemma of studying and studies is studi, where the stemma of studying is study and the stemma of studies is studi)."
      ],
      "metadata": {
        "id": "I0m6R3REKx31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(1 point)** On logistic regression:\n",
        "        How does stochastic gradient descent work?\n",
        "        What is the role of the learning rate?\n",
        "        Will it always find the global minimum?\n"
      ],
      "metadata": {
        "id": "4aD9YzC6K0at"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic gradient descent (SGD) is an optimizing method heavily used in Machine learning. It attempts to approximate an objective function by updating iteratively a set of weights in the used model. SGD does so by computing the gradient of the model's loss on a random sample of a training dataset.\n",
        "\n",
        "The learning rate impacts the \"speed\" at which our model learns. The higher its value the faster it will converge, but it will be harder to obtain an ideal representation. On the flipside, a smaller learning rate implies a slower convergence but a better final representation.\n",
        "\n",
        "Unless our case study is very simple (ex: approximating a linear function), we will not find the global minimum. however it is highly likely that we will find a good approximation of it."
      ],
      "metadata": {
        "id": "6njymY6QK4PN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(1 point)** What problems does TF-iDF try to solve?\n",
        "        What the is the TF part for?\n",
        "        What is the iDF part for?"
      ],
      "metadata": {
        "id": "FgEzofSgK6EF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-iDF is a method that tries to define the importance of each word in a document relative to a corpus of documents. It does so by comparing the frequency of a word in the document compared to its frequency in the corpus.\n",
        "\n",
        "TF stands for Term Frequency. This part computes the number of occurences of a word in a specific document.\n",
        "\n",
        "iDF stands for inverse Document Frequency. This part gives higher importance to words that occur rarely in the corpus, since they are more specific to our document and are more likely to contain distinctive information."
      ],
      "metadata": {
        "id": "t_tq7LnXLAdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(2 point)** Summarize how the skip-gram method of Word2Vec works using a couple of paragraphs.\n",
        "        How does it uses the fact that two words appearing in similar contexts are likely to have similar meanings?"
      ],
      "metadata": {
        "id": "mzrd5uUYLCMW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec tries to represent words as dense vectors. The skip-gram method used attempts to associate for each pair of words that frequently appear close to each other as words that have similar meaning.\n",
        "\n",
        "Basically we pass a context window of static size on our sequence of words. For each word we compute the scalar product of its word embedding with each other word's context embedding in the context window. The more often words are 'seen' in the same context window the closer will their embeddings be."
      ],
      "metadata": {
        "id": "Z70gD3f8LG4-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(1 point)** What are the differences between an RNN and an LSTM?\n",
        "        What problem is an LSTM trying to solve compared to a basic RNN?"
      ],
      "metadata": {
        "id": "qLYRshR_LLad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main differences between RNNs and LSTMs are the way they handle context. LSTMs use a mechanism of forgetting and adding information depending on a context layer. This helps to resolve the vanishing/exploding gradient problem that RNN often face on long sequences."
      ],
      "metadata": {
        "id": "MpwBgGMpLQD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **(1 point)** What would you expect if we use one of our classifiers trained on IMDB on Twitter data, and why?"
      ],
      "metadata": {
        "id": "d6S2TPbXLSDd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is highly likely we will lose in quality if we run our models on Twitter data for two main reasons: the quality of the vocabulary and the limited input length of twits. Since IMDB has no character limitation, users are less likely to add abbreviation and the overall tone of reviews tends to be more formulated, whereas twits contain a large amount of abbreviations and the language tends to be closer to slang."
      ],
      "metadata": {
        "id": "SCHtJzP7LTqF"
      }
    }
  ]
}